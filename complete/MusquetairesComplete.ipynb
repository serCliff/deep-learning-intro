{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating text with Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reading\n",
    "Load the file from the data folder and inspect it. Standardize to lowercase. How long is the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "datasets_path = os.path.join(os.path.dirname(os.getcwd()), 'datasets')\n",
    "musquetaires_path = os.path.join(datasets_path, 'musquetaires')\n",
    "\n",
    "filename = os.path.join(musquetaires_path, 'musquetairesShort')\n",
    "raw_text = open(filename).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 198059\n"
     ]
    }
   ],
   "source": [
    "text = raw_text.lower()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preparation\n",
    "We create a set with the different characters and two dictionaries from indices to chars\n",
    "<font color=red><b>Generate dictionaries for the char to indices and indices to chars.\n",
    "<br>_Hint: use the enumerate function on the chars set_</b>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 52\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we generate the input and output arrays:\n",
    "\n",
    "The input will consist on sentences of a fixed (_maxlen_) lenght, while the outputs will be the next characters in the text.\n",
    "\n",
    "So, if the text is \"Welcome to Big Data Spain\" with _maxlen_ = 5, we will have:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input = [\n",
    "     w, e, l, c, o,\n",
    "     e, l, c, o, m,\n",
    "     l, c, o, m, e,\n",
    "     ...\n",
    "    ]\n",
    "\n",
    "Output = [\n",
    "    m,\n",
    "    e, \n",
    "     ,\n",
    "     ...\n",
    "     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid overfitting (and improve performances) we can add a _step_ to the structure so that with step = 3, for example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input = [\n",
    "     w, e, l, c, o,\n",
    "     c, o, m, e,  , \n",
    "     m, e,  , t, o, \n",
    "     ...\n",
    "    ]\n",
    "\n",
    "Output = [\n",
    "    m,\n",
    "    t, \n",
    "     ,\n",
    "     ...\n",
    "     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red><b>Fill the sentences and next_char lists with the input and output data</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 66007\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i + maxlen])\n",
    "    next_chars.append(text[i + maxlen])\n",
    "print('nb sequences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['author’s preface\\n\\n\\nin which it is proved',\n",
       " 'hor’s preface\\n\\n\\nin which it is proved th',\n",
       " '’s preface\\n\\n\\nin which it is proved that,',\n",
       " 'preface\\n\\n\\nin which it is proved that, no',\n",
       " 'face\\n\\n\\nin which it is proved that, notwi']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ', 'a', ' ', 't', 't']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_chars[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset generation\n",
    "We turn the text into one-hot-like vectors. Initialize the Input and output arrays to zero as boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "Y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    Y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps =  40 , numchars =  52\n"
     ]
    }
   ],
   "source": [
    "print (\"timesteps = \", len (X[0]), \", numchars = \", len (X[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Generation\n",
    "Build the LSTM model to be trained train on the data, on this config:\n",
    "- LSTM layer, with 256 units\n",
    "- LSTM layer, with 256 units\n",
    "- Dense layer, with 64 units\n",
    "- Dense softmax layer\n",
    "- On compilation, use adam as the optimizer and categorical_crossentropy as the loss function.\n",
    "- Print the summary\n",
    "\n",
    "\n",
    "<font color=red><b>Remember to initialize it propperly and to include input_shape on the first layer. <br> Hints: input_shape= (maxlen, len(chars))\n",
    "- Use the imported libraries</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.keras.backend.clear_session() \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 40, 256)           316416    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               525312    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 52)                3380      \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 52)                0         \n",
      "=================================================================\n",
      "Total params: 861,556\n",
      "Trainable params: 861,556\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maxlen, len(chars)), return_sequences=True))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "Train the model for a couple of epochs and see how it works. Use a batch_size of 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66007 samples\n",
      "Epoch 1/2\n",
      " 2432/66007 [>.............................] - ETA: 7:38 - loss: 3.4267"
     ]
    }
   ],
   "source": [
    "model.fit(X, Y,\n",
    "      batch_size=128,\n",
    "      epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "Let's test our model. In order to obtain a probabilistic answer we can sample from a probability array instead of just taking the max argument:\n",
    "\n",
    "<font color=red><b> Sometimes probabilities are rounded. Apply a normalization-like tratment to them in order to avoid this when sampling</b> </font>\n",
    "\n",
    "\n",
    "$$ p_i = \\frac{p_i}{\\sum_j p_j}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, sample = True):\n",
    "    if sample:\n",
    "        # probs can be rounded and not sum up to one. We recalculate in order to avoid this\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = preds /np.sum (preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "    else:\n",
    "        probas = preds\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a seed in order to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "generated = ''\n",
    "sentence = text[start_index: start_index + maxlen]\n",
    "generated += sentence\n",
    "print (generated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions\n",
    "This will be the secuence for which we are going to predict the next character:\n",
    "\n",
    "<font color=red> <b> Predict the next character given the input x_pred. <br>Hint: remember to take the first item in list</b>  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict next character given a model and the sequence to predict \n",
    "def get_next_char (model, x_pred, indices_char, Sample = True):\n",
    "    preds = model.predict(x_pred, verbose=0)[0]\n",
    "    next_index = sample(preds, 1.0)\n",
    "    return indices_char[next_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "for t, char in enumerate(sentence):\n",
    "    x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "Sample = True\n",
    "    \n",
    "next_char = get_next_char (model, x_pred, indices_char, Sample)  \n",
    "\n",
    "print (next_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's predict some more characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "sentence = text[start_index: start_index + maxlen]\n",
    "print('Seed: ' + sentence + '\"')\n",
    "print('---------------------- Generated Text -----------------------')\n",
    "# sys.stdout.write(generated)\n",
    "for i in range(400):\n",
    "    x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "    for t, char in enumerate(sentence):\n",
    "        x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "    next_char = get_next_char (model, x_pred, indices_char, Sample)\n",
    "    sentence = sentence[1:] + next_char\n",
    "\n",
    "    sys.stdout.write(next_char)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a trained model\n",
    "Training Deep Learning Models is time consuming. So, some pretrained models are available to be loaded and take a look at better predictions. We will load a model for each 5 epochs in order to see the evolution. \n",
    "\n",
    "<font color=red> <b> Load a model for each time and predict the text <br> Hint: You can load the whole model or just the weights as the configuration is the same</b>  </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "partial_n_epoch = 5\n",
    "times = 12\n",
    "\n",
    "## Set both starts for the seed sentence and the multinomial prediction\n",
    "np.random.seed (2)\n",
    "for j in range (times):\n",
    "    \n",
    "    count += partial_n_epoch\n",
    "    print (\"\")\n",
    "    print (\"-------------- Next Model --------------\")\n",
    "    print (\"Trained on \", count, \" epochs\")\n",
    "    modelName = os.path.join(musquetaires_path,'MusquetairesModelOptimizedMode_' + str (count) + '.h5' )\n",
    "\n",
    "    model.load_weights (modelName)\n",
    "   \n",
    "    start_index = np.random.randint(0, len(text) - maxlen - 1)\n",
    "    sentence = text[start_index: start_index + maxlen]\n",
    "    print('Seed: ' + sentence + '\"')\n",
    "    print('---------------------- Generated Text -----------------------')\n",
    "    # sys.stdout.write(generated)\n",
    "    for i in range(400):\n",
    "        x_pred = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x_pred[0, t, char_indices[char]] = 1.\n",
    "\n",
    "        next_char = get_next_char (model, x_pred, indices_char, Sample)\n",
    "        sentence = sentence[1:] + next_char\n",
    "\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
