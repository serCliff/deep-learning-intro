{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://deepnotes.io/softmax-crossentropy\n",
    "\n",
    "https://mlfromscratch.com/neural-networks-explained/#backpropagation\n",
    "\n",
    "http://neuralnetworksanddeeplearning.com/chap3.html#introducing_the_cross-entropy_cost_function\n",
    "\n",
    "https://towardsdatascience.com/neural-networks-from-scratch-easy-vs-hard-b26ddc2e89c7\n",
    "    \n",
    "    \n",
    "https://stackabuse.com/creating-a-neural-network-from-scratch-in-python-multi-class-classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network from scratch\n",
    "\n",
    "In this nnotebook we will learn how to build a neural network from scratch, implementing the forward and the backward propagation and training it on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "We will cover this dataset further in future practises, but let's take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fdfcbe407f0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "dig = load_digits()\n",
    "plt.gray()\n",
    "plt.matshow(dig.images[25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple train/test split with one hot features on the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_target = pd.get_dummies(dig.target)\n",
    "x_train, x_val, y_train, y_val = train_test_split(dig.data, onehot_target, test_size=0.1, random_state=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architechture and basic operations\n",
    "We willl cover the base of the network, which will be a [128, 128, 10] fully connected network, including all the weights for each layer and its biases.\n",
    "\n",
    "**Finish implementing the basic architechture**\n",
    "\n",
    "*Weight matrixes can be initialized with rnp.random.randn, while biases could be initialised with zeros (np.zeros). Keep in mind dimensions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class MyNN:\n",
    "    def __init__(self, x, y):\n",
    "        self.input = x\n",
    "        self.target = np.array(y)\n",
    "        neurons = 128       # neurons for hidden layers\n",
    "        self.lr = 0.5       # user defined learning rate\n",
    "        ip_dim = x.shape[1] # input layer size\n",
    "        op_dim = y.shape[1] # output layer size \n",
    "        self.w1 = np.random.randn(ip_dim, neurons) # weights\n",
    "        self.b1 = np.zeros((1, neurons))           # biases\n",
    "        self.w2 = np.random.randn(neurons, neurons)\n",
    "        self.b2 = np.zeros((1, neurons))\n",
    "        self.w3 = np.random.randn(neurons, op_dim)\n",
    "        self.b3 = np.zeros((1, op_dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activations definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(s):\n",
    "    return 1/(1 + np.exp(-s))# for numerical stability, values are normalised\n",
    "def softmax(s):\n",
    "    exps = np.exp(s - np.max(s, axis=1, keepdims=True))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward and backward implementation\n",
    "On this section we will implement both the forward and the backward passes.\n",
    "## Forwad prop\n",
    "For the forward pass, these are the feed forward equations:\n",
    "$$ z1 = x\\cdot w1+b1$$\n",
    "$$ a1 = \\sigma(z1)$$ \n",
    "$$ z2 = a1\\cdot w2+b2$$\n",
    "$$ a2 = \\sigma(z2)$$\n",
    "$$ z3 = a2\\cdot w3+b3$$\n",
    "$$ a3 = softmax(z3)$$\n",
    "\n",
    "**Implement a methof for the forward pass**\n",
    "\n",
    "*Use np.dot for the matrix product. Keep in mind our activations will be mainly sigmoid and a softmax in the end*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN:\n",
    "    def __init__(self, x, y):\n",
    "        self.input = x\n",
    "        self.target = np.array(y)\n",
    "        neurons = 128       # neurons for hidden layers\n",
    "        self.lr = 0.5       # user defined learning rate\n",
    "        ip_dim = x.shape[1] # input layer size\n",
    "        op_dim = y.shape[1] # output layer size \n",
    "        self.w1 = np.random.randn(ip_dim, neurons) # weights\n",
    "        self.b1 = np.zeros((1, neurons))           # biases\n",
    "        self.w2 = np.random.randn(neurons, neurons)\n",
    "        self.b2 = np.zeros((1, neurons))\n",
    "        self.w3 = np.random.randn(neurons, op_dim)\n",
    "        self.b3 = np.zeros((1, op_dim))\n",
    "        \n",
    "    def feedforward(self):\n",
    "        z1 = np.dot(self.input, self.w1) + self.b1\n",
    "        self.a1 = sigmoid(z1)\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = sigmoid(z2)\n",
    "        z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "        self.a3 = softmax(z3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "np.random.seed(0)\n",
    "model = MyNN(x_train, y_train)\n",
    "model.feedforward()\n",
    "epsilon = 1e-5\n",
    "np.max (model.a3[0] - np.array([1.49799065e-09, 6.42638933e-06, 4.51090759e-11, 3.39927186e-05,\n",
    "       7.78156672e-06, 7.53979236e-07, 7.12601701e-08, 7.56422143e-14,\n",
    "       9.99950963e-01, 9.48625185e-09])) < epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation\n",
    "### Chain — rule:\n",
    "\n",
    "Let us consider the cost function as c, From the feed forward, we know that\n",
    "\n",
    "$$z = xw + b                 ==> z = function(w)$$\n",
    "\n",
    "$$a = activation(z)  ==> a = function(z)$$\n",
    "\n",
    "$$c = -(y*log(a))           ==> c = function(a)$$\n",
    "\n",
    "Thus, from the output all we have to do is find the error and how much does each weights influences the output. In other words, find the derivative of cost function w.r.t w3. Yes, Back propagation is nothing but calculation of derivatives using chain rule.\n",
    "\n",
    "###  Outer layer\n",
    "$$\\frac{dc}{dw3} = \\frac{dc}{da3}\\cdot \\frac{da3}{dz3}\\cdot \\frac{dz3}{dw3}$$ \n",
    "\n",
    "$$z3 = a2w3 + b3$$\n",
    "$$a3 = softmax(z3)$$\n",
    "\n",
    "$$\\frac{dz3}{dw3} = a2$$\n",
    "\n",
    "$$\\frac{da3}{dz3} = \\frac{d softmax(z3)}{d z3}$$\n",
    "\n",
    "Surprisingly, as cross-entropy is often used with softmax activation function, we do not really have to compute both of these derivatives. Because, some of the parts of these derivatives cancel each other as clearly explained [here](https://deepnotes.io/softmax-crossentropy). Thus, predicted value — real value is the result of their product.\n",
    "\n",
    "$$\\frac{dc}{da3}  = \\frac{d c}{d a3} = \\frac{-y}{a3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let, $a3_\\delta$ be the product of these terms as it will be needed in the upcoming chain rules.\n",
    "$$ a3_\\delta = \\frac{dc}{da3}\\cdot\\frac{da3}{dz3} $$\n",
    "Thus, $a3_\\delta = a3-y$ (the error to be propagated)\n",
    "$$ \\frac{dc}{dw3} = (a3 - y) \\cdot a2$$\n",
    "$$ \\text{Update: }w3 := w3 - \\frac{dc}{dw3}$$\n",
    "For changes in biases,\n",
    "$$ \\frac{dc}{db3} = \\frac{dc}{da3}\\cdot \\frac{da3}{dz3}\\cdot \\frac{dz3}{db3} $$\n",
    "$\\frac{dz3}{db3} = 1$ . Rest is already calculated:\n",
    "$ b3 = b3 - \\frac{dc}{db3} => \\text{Update: } b3 := b3 - a3_\\delta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layers:\n",
    "\n",
    "In hidden layers, the partial derivative of cost function w.r.t hidden layers will also follow a chain rule, since cost function is a function of outer layer.\n",
    "\n",
    "$$z2 = a1w2 + b2$$\n",
    "$$ a2 = sigmoid(z2)$$\n",
    "$$ \\frac{dc}{dw2}  =   \\frac{dc}{da2} \\cdot \\frac{da2}{dz2}\\cdot \\frac{dz2}{dw2}$$\n",
    "$$ \\frac{dz2}{dw2}= a1$$\n",
    "$$ \\frac{da2}{dz2} = \\sigma ' (z2)$$\n",
    "$$ \\frac{dc}{da2} = \\frac{dc}{da3}\\cdot \\frac{da3}{dz3}\\cdot \\frac{dz3}{da2} => \\frac{dc}{da2} = a3_\\delta \\cdot w3 $$\n",
    "$$ \\text{Update: } w2 := w2 - \\frac{dc}{dw2}$$\n",
    "$$ \\text{and set } a2_\\delta = \\frac{dc}{da2} \\cdot \\frac{da2}{dz2}$$\n",
    "$$ \\frac{dc}{db2}= \\frac{dc}{da2}\\cdot \\frac{da2}{dz2}\\cdot \\frac{dz2}{db2} $$\n",
    "$$ \\frac{dz2}{db2} = 1; \\text{Update: } b2 := b2 - dc/db2 => b2 := b2 - a2_\\delta$$\n",
    "Similarly for w1 and b1.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back propagation equations:\n",
    "$$ a3_\\delta = a3-y $$ \n",
    "$$ z2_\\delta = a3_\\delta\\cdot w3^T$$\n",
    "$$ a2_\\delta = z2_\\delta\\cdot \\sigma '(a2)$$\n",
    "$$ z1_\\delta = a2_\\delta\\cdot w2^T$$\n",
    "$$ a1_\\delta = z1_\\delta\\cdot\\sigma '(a1)$$\n",
    "\n",
    "**Implement a method for the backpropagation**\n",
    "\n",
    "*You can use the implemented functions for sigmoid derivative and error computation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derv(s):\n",
    "    return s * (1 - s)\n",
    "def error(pred, real):\n",
    "    n_samples = real.shape[0]\n",
    "    res = pred - real\n",
    "    return res/n_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN:    \n",
    "    def __init__(self, x, y):\n",
    "        self.input = x\n",
    "        self.target = np.array(y)\n",
    "        neurons = 128       # neurons for hidden layers\n",
    "        self.lr = 0.5       # user defined learning rate\n",
    "        ip_dim = x.shape[1] # input layer size\n",
    "        op_dim = y.shape[1] # output layer size \n",
    "        self.w1 = np.random.randn(ip_dim, neurons) # weights\n",
    "        self.b1 = np.zeros((1, neurons))           # biases\n",
    "        self.w2 = np.random.randn(neurons, neurons)\n",
    "        self.b2 = np.zeros((1, neurons))\n",
    "        self.w3 = np.random.randn(neurons, op_dim)\n",
    "        self.b3 = np.zeros((1, op_dim))\n",
    "    def feedforward(self):\n",
    "        z1 = np.dot(self.input, self.w1) + self.b1\n",
    "        self.a1 = sigmoid(z1)\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = sigmoid(z2)\n",
    "        z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "        self.a3 = softmax(z3)\n",
    "    def backprop(self):\n",
    "        a3_delta = error(self.a3, self.target) # w3\n",
    "        z2_delta = np.dot(a3_delta, self.w3.T)\n",
    "        a2_delta = z2_delta * sigmoid_derv(self.a2) # w2\n",
    "        z1_delta = np.dot(a2_delta, self.w2.T)\n",
    "        a1_delta = z1_delta * sigmoid_derv(self.a1) # w1\n",
    "        return a1_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "np.random.seed(0)\n",
    "model = MyNN(x_train, y_train)\n",
    "model.feedforward()\n",
    "np.max (model.backprop()[0] - np.array([ 6.92440369e-12, -3.66641945e-42, -1.25395442e-09,  3.93615869e-12,\n",
    "       -4.97931120e-23, -1.85093629e-64,  0.00000000e+00,  3.23685646e-24,\n",
    "       -5.45518366e-32,  0.00000000e+00, -0.00000000e+00, -7.54331820e-09,\n",
    "       -3.73586429e-19,  1.50686055e-63,  0.00000000e+00, -4.79315252e-19,\n",
    "       -2.54916838e-10, -1.35810376e-09,  1.94242534e-25,  0.00000000e+00,\n",
    "        5.17322826e-34,  8.81966807e-06, -8.59520406e-05, -1.39608578e-04,\n",
    "       -0.00000000e+00,  5.27884470e-39, -2.96384906e-42,  5.30508535e-17,\n",
    "       -0.00000000e+00,  4.32544729e-31, -3.53209368e-17, -5.56179839e-20,\n",
    "        4.88313935e-32, -6.76705842e-08,  0.00000000e+00,  3.52885333e-11,\n",
    "       -9.36113810e-28,  0.00000000e+00,  1.10918779e-29, -6.11964244e-10,\n",
    "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -1.12098629e-14,\n",
    "        5.21673593e-36,  0.00000000e+00,  0.00000000e+00,  1.87388028e-55,\n",
    "        5.77760552e-13,  3.73529444e-13, -5.35449680e-21, -0.00000000e+00,\n",
    "        0.00000000e+00,  0.00000000e+00, -5.50503173e-12,  1.53023520e-09,\n",
    "        2.58805646e-29, -1.21673827e-08, -0.00000000e+00,  3.46664501e-17,\n",
    "        4.13698326e-29, -0.00000000e+00, -2.49447056e-12,  0.00000000e+00,\n",
    "        3.03057778e-17,  0.00000000e+00,  6.10754324e-16,  0.00000000e+00,\n",
    "       -0.00000000e+00,  0.00000000e+00, -4.36812227e-06, -0.00000000e+00,\n",
    "       -2.60380528e-10, -2.09547818e-05, -0.00000000e+00,  0.00000000e+00,\n",
    "       -3.22357991e-48,  9.75311266e-52, -1.24217411e-14, -1.60570557e-05,\n",
    "        1.14243867e-26,  0.00000000e+00, -2.18627776e-31,  9.46194493e-37,\n",
    "       -1.29093851e-36, -6.31138244e-26,  1.87196067e-51, -3.22749984e-20,\n",
    "       -1.12301137e-23, -1.07727191e-15,  3.18798435e-14, -7.17992952e-10,\n",
    "       -2.33406393e-41, -1.46456277e-13,  9.58206251e-21,  4.14870581e-60,\n",
    "        6.24451994e-21, -1.03407652e-14,  1.25867936e-29,  3.04082209e-19,\n",
    "       -0.00000000e+00,  8.20891326e-33,  2.06647354e-16, -0.00000000e+00,\n",
    "        5.52075138e-34,  0.00000000e+00, -0.00000000e+00, -8.89175016e-10,\n",
    "        3.00779906e-11, -0.00000000e+00, -2.70979552e-16,  0.00000000e+00,\n",
    "        0.00000000e+00, -3.08725438e-32, -9.43289939e-21, -7.52646867e-16,\n",
    "       -5.58410094e-16, -0.00000000e+00,  2.76331231e-61, -1.52627365e-69,\n",
    "       -6.77206286e-10,  8.15621692e-13,  1.78175086e-43,  0.00000000e+00,\n",
    "        6.12727266e-29, -7.87530664e-05, -1.52532862e-41, -2.76926091e-25])) < epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Loss: Cross Entropy\n",
    "We will use cross entropy for the loss computation:\n",
    "$$\\mathcal{L} = - \\frac{1}{n}\\cdot\\sum y_i\\cdot log(\\widehat{y_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred, real):\n",
    "    n_samples = real.shape[0]\n",
    "    logp = - np.log(pred[np.arange(n_samples), real.argmax(axis=1)])\n",
    "    loss = np.sum(logp)/n_samples\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updates\n",
    "On each pass, the weight and biases need to be updated. **Add the update inside the backprop method. Add a predict method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN:    \n",
    "    def __init__(self, x, y):\n",
    "        self.input = x\n",
    "        self.target = np.array(y)\n",
    "        neurons = 128       # neurons for hidden layers\n",
    "        self.lr = 0.5       # user defined learning rate\n",
    "        ip_dim = x.shape[1] # input layer size\n",
    "        op_dim = y.shape[1] # output layer size \n",
    "        self.w1 = np.random.randn(ip_dim, neurons) # weights\n",
    "        self.b1 = np.zeros((1, neurons))           # biases\n",
    "        self.w2 = np.random.randn(neurons, neurons)\n",
    "        self.b2 = np.zeros((1, neurons))\n",
    "        self.w3 = np.random.randn(neurons, op_dim)\n",
    "        self.b3 = np.zeros((1, op_dim))\n",
    "    def feedforward(self):\n",
    "        z1 = np.dot(self.input, self.w1) + self.b1\n",
    "        self.a1 = sigmoid(z1)\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.a2 = sigmoid(z2)\n",
    "        z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "        self.a3 = softmax(z3)\n",
    "    def predict(self, data):\n",
    "        self.input = data\n",
    "        self.feedforward()\n",
    "        return self.a3.argmax(axis=1)\n",
    "    def backprop(self):\n",
    "        a3_delta = error(self.a3, self.target) # w3\n",
    "        z2_delta = np.dot(a3_delta, self.w3.T)\n",
    "        a2_delta = z2_delta * sigmoid_derv(self.a2) # w2\n",
    "        z1_delta = np.dot(a2_delta, self.w2.T)\n",
    "        a1_delta = z1_delta * sigmoid_derv(self.a1) # w1    \n",
    "        self.w3 -= self.lr * np.dot(self.a2.T, a3_delta)\n",
    "        self.b3 -= self.lr * np.sum(a3_delta, axis=0, keepdims=True)\n",
    "        self.w2 -= self.lr * np.dot(self.a1.T, a2_delta)\n",
    "        self.b2 -= self.lr * np.sum(a2_delta, axis=0)\n",
    "        self.w1 -= self.lr * np.dot(self.input.T, a1_delta)\n",
    "        self.b1 -= self.lr * np.sum(a1_delta, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check\n",
    "np.random.seed(0)\n",
    "model = MyNN(x_train, y_train)\n",
    "model.feedforward()\n",
    "model.backprop()\n",
    "np.max (model.w1[0] - np.array([ 1.76405235,  0.40015721,  0.97873798,  2.2408932 ,  1.86755799,\n",
    "       -0.97727788,  0.95008842, -0.15135721, -0.10321885,  0.4105985 ,\n",
    "        0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n",
    "        0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574,\n",
    "       -2.55298982,  0.6536186 ,  0.8644362 , -0.74216502,  2.26975462,\n",
    "       -1.45436567,  0.04575852, -0.18718385,  1.53277921,  1.46935877,\n",
    "        0.15494743,  0.37816252, -0.88778575, -1.98079647, -0.34791215,\n",
    "        0.15634897,  1.23029068,  1.20237985, -0.38732682, -0.30230275,\n",
    "       -1.04855297, -1.42001794, -1.70627019,  1.9507754 , -0.50965218,\n",
    "       -0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028,\n",
    "       -0.89546656,  0.3869025 , -0.51080514, -1.18063218, -0.02818223,\n",
    "        0.42833187,  0.06651722,  0.3024719 , -0.63432209, -0.36274117,\n",
    "       -0.67246045, -0.35955316, -0.81314628, -1.7262826 ,  0.17742614,\n",
    "       -0.40178094, -1.63019835,  0.46278226, -0.90729836,  0.0519454 ,\n",
    "        0.72909056,  0.12898291,  1.13940068, -1.23482582,  0.40234164,\n",
    "       -0.68481009, -0.87079715, -0.57884966, -0.31155253,  0.05616534,\n",
    "       -1.16514984,  0.90082649,  0.46566244, -1.53624369,  1.48825219,\n",
    "        1.89588918,  1.17877957, -0.17992484, -1.07075262,  1.05445173,\n",
    "       -0.40317695,  1.22244507,  0.20827498,  0.97663904,  0.3563664 ,\n",
    "        0.70657317,  0.01050002,  1.78587049,  0.12691209,  0.40198936,\n",
    "        1.8831507 , -1.34775906, -1.270485  ,  0.96939671, -1.17312341,\n",
    "        1.94362119, -0.41361898, -0.74745481,  1.92294203,  1.48051479,\n",
    "        1.86755896,  0.90604466, -0.86122569,  1.91006495, -0.26800337,\n",
    "        0.8024564 ,  0.94725197, -0.15501009,  0.61407937,  0.92220667,\n",
    "        0.37642553, -1.09940079,  0.29823817,  1.3263859 , -0.69456786,\n",
    "       -0.14963454, -0.43515355,  1.84926373])) < epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit loop\n",
    "Finally, we will train our neural network. After defining the NN, for each epoch we will:\n",
    "    - Forwaard pass\n",
    "    - Evaluate perf (use accuracy implemented function)\n",
    "    - Backward pass\n",
    "    \n",
    "**Implement the fit loop**\n",
    "\n",
    "*Please remember to evaluate first validation and then training due to nn implementation on data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  10.501475416224613  . Training accuracy:  0.12430426716141002\n",
      "Validation accuracy:  0.10555555555555556\n",
      "Training loss:  6.76168094675108  . Training accuracy:  0.12368583797155226\n",
      "Validation accuracy:  0.12777777777777777\n",
      "Training loss:  4.955787248961067  . Training accuracy:  0.18119975262832405\n",
      "Validation accuracy:  0.2\n",
      "Training loss:  3.7418263682005373  . Training accuracy:  0.21150278293135436\n",
      "Validation accuracy:  0.17222222222222222\n",
      "Training loss:  2.841755654711245  . Training accuracy:  0.2714904143475572\n",
      "Validation accuracy:  0.2833333333333333\n",
      "Training loss:  2.3736939055887367  . Training accuracy:  0.33518862090290663\n",
      "Validation accuracy:  0.3055555555555556\n",
      "Training loss:  2.0411969303164312  . Training accuracy:  0.4007421150278293\n",
      "Validation accuracy:  0.38333333333333336\n",
      "Training loss:  1.863691336434309  . Training accuracy:  0.4217687074829932\n",
      "Validation accuracy:  0.3888888888888889\n",
      "Training loss:  1.7633835719725912  . Training accuracy:  0.4681508967223253\n",
      "Validation accuracy:  0.43333333333333335\n",
      "Training loss:  1.662993172959776  . Training accuracy:  0.48794063079777367\n",
      "Validation accuracy:  0.4388888888888889\n",
      "Training loss:  1.555192875248743  . Training accuracy:  0.520717377860235\n",
      "Validation accuracy:  0.4722222222222222\n",
      "Training loss:  1.478356829419913  . Training accuracy:  0.5448361162646876\n",
      "Validation accuracy:  0.4888888888888889\n",
      "Training loss:  1.3605374282042277  . Training accuracy:  0.5664811379097093\n",
      "Validation accuracy:  0.4888888888888889\n",
      "Training loss:  1.3134137020714252  . Training accuracy:  0.5943104514533086\n",
      "Validation accuracy:  0.5166666666666667\n",
      "Training loss:  1.1943730862966808  . Training accuracy:  0.6017316017316018\n",
      "Validation accuracy:  0.5277777777777778\n",
      "Training loss:  1.151620320893768  . Training accuracy:  0.6444032158317873\n",
      "Validation accuracy:  0.5444444444444444\n",
      "Training loss:  1.063053311659115  . Training accuracy:  0.6413110698824984\n",
      "Validation accuracy:  0.55\n",
      "Training loss:  1.0372637763392791  . Training accuracy:  0.6827458256029685\n",
      "Validation accuracy:  0.5555555555555556\n",
      "Training loss:  0.9673475043911465  . Training accuracy:  0.6747062461348176\n",
      "Validation accuracy:  0.5888888888888889\n",
      "Training loss:  0.9760698328216733  . Training accuracy:  0.7087198515769945\n",
      "Validation accuracy:  0.5888888888888889\n",
      "Training loss:  0.8954198728900805  . Training accuracy:  0.7025355596784169\n",
      "Validation accuracy:  0.6\n",
      "Training loss:  0.9070458942939595  . Training accuracy:  0.7285095856524428\n",
      "Validation accuracy:  0.6\n",
      "Training loss:  0.8307722551790084  . Training accuracy:  0.7229437229437229\n",
      "Validation accuracy:  0.65\n",
      "Training loss:  0.8427387646829313  . Training accuracy:  0.7445887445887446\n",
      "Validation accuracy:  0.6166666666666667\n",
      "Training loss:  0.7690924568214657  . Training accuracy:  0.7476808905380334\n",
      "Validation accuracy:  0.6611111111111111\n",
      "Training loss:  0.7772225419462032  . Training accuracy:  0.7711811997526283\n",
      "Validation accuracy:  0.6388888888888888\n",
      "Training loss:  0.710795834789108  . Training accuracy:  0.7711811997526283\n",
      "Validation accuracy:  0.6777777777777778\n",
      "Training loss:  0.715349750891332  . Training accuracy:  0.7854050711193569\n",
      "Validation accuracy:  0.65\n",
      "Training loss:  0.659489913435785  . Training accuracy:  0.7915893630179345\n",
      "Validation accuracy:  0.7\n",
      "Training loss:  0.6617995131677211  . Training accuracy:  0.7983920841063699\n",
      "Validation accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_accuracy(y, y_hat):\n",
    "    return accuracy_score(y_hat, y.apply (lambda x: np.argmax(x), axis = 1).values)\n",
    "\n",
    "model = MyNN(x_train, y_train)\n",
    "num_epochs = 30\n",
    "for j in range(num_epochs): \n",
    "    model.feedforward()\n",
    "    pred = model.a3\n",
    "    loss = cross_entropy(pred, model.target)\n",
    "    val_acc = evaluate_accuracy(y_val, model.predict(x_val))\n",
    "    train_acc = evaluate_accuracy(y_train, model.predict(x_train))\n",
    "    print('Training loss: ', loss,  ' . Training accuracy: ', train_acc)\n",
    "    print('Validation accuracy: ', val_acc)\n",
    "    model.backprop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_tf2",
   "language": "python",
   "name": "dl_tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
